import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../"))

# Add the project root to the Python path so Python can find the modules directory
sys.path.append(project_root)

# Now you can import from modules
from agents import agents
import time
import random
import csv
import traceback
import pandas as pd
from fake_useragent import UserAgent
from datetime import datetime
import undetected_chromedriver as uc
import re

from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.common.keys import Keys
from selenium.webdriver import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# =========================================================================
# CONFIGURACI√ìN GENERAL
# =========================================================================

# Credenciales de Instagram *incrustadas* directamente
INSTA_USER = "ignacio.lopez@wivboost.com"
INSTA_PASS = "buenosdiasAlegria777"

# Cuenta a extraer - IMPORTANTE: c√°mbialo a la cuenta deseada
ACCOUNT = "feria de sevilla"

# A√±o m√≠nimo a filtrar
YEAR_FILTER = 2025
MAX_POSTS = 47 # N√∫mero m√°ximo de posts a extraer
NUM_SCROLLS = 10  # Aumentado para cargar m√°s posts

# =========================================================================
# CLASE SCRAPER
# =========================================================================
class InstagramScraper:
    def __init__(self):
        self.driver = self._setup_driver()
        self.action = ActionChains(self.driver)
        self.debug_mode = True  # Activa el modo debug para m√°s informaci√≥n

    def _setup_driver(self):
        """
        Configura Selenium con el proxy residencial de Bright Data.
        """
        PROXY_HOST = "gate.smartproxy.com"
        PROXY_PORT = "1005"
        PROXY_USER = "sp03mahcda"
        PROXY_PASS = "X3s_awrkk90gNbs0YX"

        proxy_options = {
            "proxy": {
                "http": f"http://{PROXY_USER}:{PROXY_PASS}@{PROXY_HOST}:{PROXY_PORT}",
                "https": f"http://{PROXY_USER}:{PROXY_PASS}@{PROXY_HOST}:{PROXY_PORT}",
                "no_proxy": "localhost,127.0.0.1",
            }
        }
        
        # Configuraci√≥n del navegador Chrome
        options = uc.ChromeOptions()
        
        # üîí Evita que los sitios detecten Selenium
        options.add_argument("--disable-blink-features=AutomationControlled")

        # ‚ö° Optimiza el rendimiento en servidores sin interfaz gr√°fica
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")

        # üñ•Ô∏è Simula navegaci√≥n real en una ventana maximizada
        options.add_argument("--start-maximized")
        
        # NO USAR HEADLESS MODE - Causa problemas
        # options.add_argument("--headless=new")

        # üîï Evita notificaciones emergentes y bloqueos de pop-ups
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")

        # üîÑ Cambia din√°micamente el User-Agent para evitar detecci√≥n
        options.add_argument(f"--user-agent={self._random_user_agent()}")

        # üîç Evita filtraci√≥n de IP real cuando se usan proxies
        options.add_argument("--disable-webrtc")
        
        # üöÄ Inicializa el navegador sin subprocess
        driver = uc.Chrome(options=options)

        # Reducido el tiempo de espera para mayor velocidad
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(5)
        return driver

    def _random_user_agent(self):
        """
        Devuelve un User-Agent aleatorio para evitar detecci√≥n.
        """
        return random.choice(agents)
        
    def _human_delay(self, min_s=0.5, max_s=1.5):
        """
        Pausa aleatoria entre acciones para simular comportamiento humano.
        Reducido para mayor velocidad.
        """
        base = random.uniform(min_s, max_s)
        gauss_factor = random.gauss(0, 0.2)
        total = max(0, base + gauss_factor)
        time.sleep(total)

    def login(self):
        """
        Inicia sesi√≥n en Instagram con las credenciales.
        """
        print("[INFO] Iniciando proceso de login...")
        self.driver.get("https://www.instagram.com/")
        self._human_delay(3, 5)  # Reducido para mayor velocidad

        try:
            # Aceptar cookies (m√∫ltiples intentos con diferentes textos)
            cookie_buttons = [
                "//button[contains(., 'Permitir')]", 
                "//button[contains(., 'Accept')]",
                "//button[contains(., 'Allow')]",
                "//button[contains(., 'Aceptar')]"
            ]
            
            for btn_xpath in cookie_buttons:
                try:
                    allow_btn = WebDriverWait(self.driver, 2).until(
                        EC.element_to_be_clickable((By.XPATH, btn_xpath))
                    )
                    allow_btn.click()
                    self._human_delay(0.5, 1)
                    print("[INFO] Cookies aceptadas.")
                    break
                except:
                    continue
                    
        except:
            print("[INFO] No se detect√≥ di√°logo de cookies o ya fueron aceptadas.")

        try:
            # Esperar a que los campos de login est√©n disponibles
            print("[INFO] Buscando campos de login...")
            username_input = WebDriverWait(self.driver, 5).until(
                EC.presence_of_element_located((By.NAME, "username"))
            )
            password_input = self.driver.find_element(By.NAME, "password")

            print("[INFO] Campos de login encontrados, ingresando credenciales...")
            # Escribir usuario y contrase√±a m√°s r√°pido
            username_input.send_keys(INSTA_USER)
            self._human_delay(0.2, 0.5)
            password_input.send_keys(INSTA_PASS)

            # Click en "Iniciar sesi√≥n"
            print("[INFO] Buscando bot√≥n de login...")
            login_btn = WebDriverWait(self.driver, 3).until(
                EC.element_to_be_clickable((By.XPATH, "//button[@type='submit']"))
            )
            print("[INFO] Haciendo clic en bot√≥n de login...")
            login_btn.click()

            # Esperar m√°s tiempo para el login
            print("[INFO] Esperando el proceso de login...")
            self._human_delay(5, 8)  # Reducido para mayor velocidad
            
            # Verificar si hay di√°logos adicionales post-login y manejarlos
            try:
                # Bot√≥n "Not Now" para guardar informaci√≥n
                print("[INFO] Buscando di√°logos post-login...")
                not_now_btn = WebDriverWait(self.driver, 3).until(
                    EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Not Now')]"))
                )
                print("[INFO] Encontrado di√°logo 'Not Now', haciendo clic...")
                not_now_btn.click()
                self._human_delay(1, 2)
            except:
                print("[INFO] No se encontr√≥ di√°logo 'Not Now' para guardar info.")
                
            try:
                # Bot√≥n "Not Now" para notificaciones
                not_now_notif = WebDriverWait(self.driver, 3).until(
                    EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Not Now')]"))
                )
                print("[INFO] Encontrado di√°logo de notificaciones, haciendo clic en 'Not Now'...")
                not_now_notif.click()
                self._human_delay(1, 2)
            except:
                print("[INFO] No se encontr√≥ di√°logo de notificaciones.")

            # Verificar si el login fue exitoso
            if "login" in self.driver.current_url:
                print("[ERROR] La URL sigue conteniendo 'login', posible error de autenticaci√≥n.")
                raise Exception("Error de autenticaci√≥n (quiz√°s 2FA o captcha).")
            else:
                print("[INFO] Login completado correctamente.")
        except Exception as e:
            print("[ERROR] en login:", e)
            traceback.print_exc()
            raise

    def _scroll_n_times(self, times=12, pause_every=3):
        """
        Realiza scroll hacia abajo de forma r√°pida para cargar m√°s posts.
        """
        if times <= 0:
            print("üîï [INFO] Skipping scrolling as times is set to 0.")
            return 
        
        # Realizar scrolls r√°pidos
        for i in range(1, times + 1):
            print(f"üîΩ [INFO] Scroll {i}/{times}...")
            # Usar JavaScript para scrollear m√°s r√°pido
            self.driver.execute_script("window.scrollBy(0, 1000);")
            self._human_delay(0.8, 1.8)  # Ligeramente m√°s largo para asegurar carga
            
            # Cada pocos scrolls, pausa m√°s larga para carga
            if i % pause_every == 0:
                print("[INFO] Pausa para carga de contenido...")
                time.sleep(3)  # Pausa para carga de contenido
                
                # Verificar si se han cargado posts
                if self.debug_mode:
                    # Buscar elementos que parecen posts
                    articles = self.driver.find_elements(By.TAG_NAME, "article")
                    print(f"[DEBUG] Art√≠culos en la p√°gina: {len(articles)}")
                    
                    # Intentar encontrar contenedores de im√°genes/media
                    media_containers = self.driver.find_elements(By.XPATH, "//div[contains(@style, 'padding-bottom')]")
                    print(f"[DEBUG] Posibles contenedores de media: {len(media_containers)}")

    def _get_post_urls_robust(self, username, max_urls=MAX_POSTS):
        """
        M√©todo mejorado y robusto para encontrar URLs de posts de Instagram.
        Utiliza m√∫ltiples estrategias para detectar enlaces a posts.
        """
        post_urls = []
        
        print("[INFO] Buscando enlaces a posts con m√∫ltiples m√©todos...")
        
        # M√©todo 1: Buscar directamente enlaces que contengan /p/ o /reel/
        print("[INFO] M√©todo 1: Buscando enlaces directos a posts/reels...")
        direct_links = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/p/') or contains(@href, '/reel/')]")
        
        for link in direct_links:
            try:
                href = link.get_attribute("href")
                if href and (("/p/" in href) or ("/reel/" in href)):
                    # Verificamos si pertenece a la cuenta que estamos scrapeando
                    if f"instagram.com/{username.lower()}/" in href.lower() or f"/{username.lower()}/" in href.lower():
                        post_urls.append(href)
                        if self.debug_mode:
                            print(f"[DEBUG] Encontrado enlace directo: {href}")
            except Exception as e:
                if self.debug_mode:
                    print(f"[DEBUG] Error al procesar enlace: {e}")
                continue
                
        print(f"[INFO] Encontrados {len(post_urls)} enlaces directos a posts/reels")
        
        # Si no encontramos suficientes enlaces con el m√©todo 1, probamos el m√©todo 2
        if len(post_urls) < max_urls:
            print("[INFO] M√©todo 2: Buscando enlaces usando art√≠culos...")
            
            # Buscar todos los art√≠culos (suelen contener posts)
            articles = self.driver.find_elements(By.TAG_NAME, "article")
            
            for article in articles:
                try:
                    # Buscar enlaces dentro de art√≠culos
                    article_links = article.find_elements(By.TAG_NAME, "a")
                    for link in article_links:
                        href = link.get_attribute("href")
                        if href and (("/p/" in href) or ("/reel/" in href)):
                            if f"instagram.com/{username.lower()}/" in href.lower() or f"/{username.lower()}/" in href.lower():
                                if href not in post_urls:  # Evitar duplicados
                                    post_urls.append(href)
                                    if self.debug_mode:
                                        print(f"[DEBUG] Encontrado enlace en art√≠culo: {href}")
                except Exception as e:
                    if self.debug_mode:
                        print(f"[DEBUG] Error al procesar art√≠culo: {e}")
                    continue
        
        # M√©todo 3: Analizar el HTML completo de la p√°gina si a√∫n necesitamos m√°s enlaces
        if len(post_urls) < max_urls:
            print("[INFO] M√©todo 3: Analizando HTML completo...")
            
            # Obtener el HTML completo de la p√°gina
            html = self.driver.page_source
            
            # Buscar todos los enlaces a posts/reels usando expresiones regulares
            pattern1 = f'href="(https?://(?:www\\.)?instagram\\.com/{username}/(?:p|reel)/[^"]+)"'
            pattern2 = f'href="(/{username}/(?:p|reel)/[^"]+)"'
            
            links1 = re.findall(pattern1, html, re.IGNORECASE)
            links2 = re.findall(pattern2, html, re.IGNORECASE)
            
            # Convertir enlaces relativos a absolutos
            for link in links2:
                full_link = f"https://www.instagram.com{link}"
                if full_link not in post_urls:
                    post_urls.append(full_link)
                    if self.debug_mode:
                        print(f"[DEBUG] Encontrado enlace en HTML (relativo): {full_link}")
            
            # A√±adir enlaces absolutos encontrados
            for link in links1:
                if link not in post_urls:
                    post_urls.append(link)
                    if self.debug_mode:
                        print(f"[DEBUG] Encontrado enlace en HTML (absoluto): {link}")
        
        # M√©todo 4: Si todo lo anterior falla, como √∫ltimo recurso, buscar divs que parecen contener posts
        if len(post_urls) < max_urls:
            print("[INFO] M√©todo 4: Buscando divs de posts...")
            
            # En Instagram, los posts suelen tener un contenedor con padding-bottom para mantener la relaci√≥n de aspecto
            divs = self.driver.find_elements(By.XPATH, "//div[contains(@style, 'padding-bottom')]")
            
            for div in divs:
                try:
                    # Intentar encontrar el enlace m√°s cercano
                    parent = div
                    for _ in range(5):  # Buscar hasta 5 niveles arriba
                        if parent:
                            parent = parent.find_element(By.XPATH, "..")
                            links = parent.find_elements(By.TAG_NAME, "a")
                            for link in links:
                                href = link.get_attribute("href")
                                if href and (("/p/" in href) or ("/reel/" in href)):
                                    if f"instagram.com/{username.lower()}/" in href.lower() or f"/{username.lower()}/" in href.lower():
                                        if href not in post_urls:
                                            post_urls.append(href)
                                            if self.debug_mode:
                                                print(f"[DEBUG] Encontrado enlace en div: {href}")
                                            break
                except Exception as e:
                    if self.debug_mode:
                        print(f"[DEBUG] Error buscando enlaces en divs: {e}")
                    continue
        
        # Eliminar duplicados manteniendo el orden
        unique_post_urls = []
        seen = set()
        for url in post_urls:
            if url not in seen:
                seen.add(url)
                unique_post_urls.append(url)
        
        print(f"[INFO] Encontrados {len(unique_post_urls)} URLs √∫nicos de posts para {username}")
        
        # Limitar al m√°ximo n√∫mero de posts
        return unique_post_urls[:max_urls]

    def scrape_profile(self, username):
        """Extrae posts del perfil de Instagram especificado utilizando navegaci√≥n directa."""
        print(f"\nüîç [INFO] Abriendo perfil de {username}...")
        self.driver.get(f"https://www.instagram.com/{username}/")
        self._human_delay(2, 4)  # Reducido para mayor velocidad
        
        # Verificar que la p√°gina carg√≥ correctamente
        try:
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            print("[INFO] P√°gina de perfil cargada.")
            
            # Mostrar t√≠tulo y URL para diagn√≥stico
            page_title = self.driver.title
            page_url = self.driver.current_url
            print(f"[INFO] T√≠tulo de la p√°gina: {page_title}")
            print(f"[INFO] URL actual: {page_url}")
            
            # Guardar captura de la p√°gina inicial
            self.driver.save_screenshot("profile_page.png")
            print("[INFO] Captura del perfil guardada como profile_page.png")
            
        except TimeoutException:
            print("[ERROR] No se pudo cargar el perfil correctamente.")
            return []
            
        # Verificar si la cuenta es privada
        try:
            private_text = self.driver.find_element(By.XPATH, "//*[contains(text(), 'This Account is Private') or contains(text(), 'Esta cuenta es privada')]")
            print(f"‚ö†Ô∏è [WARNING] La cuenta {username} es privada. No se pueden extraer posts.")
            return []
        except NoSuchElementException:
            print(f"[INFO] La cuenta {username} parece ser p√∫blica.")
            
        # Realizar scrolls r√°pidos para cargar m√°s posts
        self._scroll_n_times(NUM_SCROLLS)
        
        # Guardar captura despu√©s de scrolls
        self.driver.save_screenshot("after_scrolls.png")
        print("[INFO] Captura despu√©s de scrolls guardada como after_scrolls.png")

        # Usar el m√©todo robusto para obtener URLs de posts
        post_urls = self._get_post_urls_robust(username, MAX_POSTS)
        
        if not post_urls:
            print(f"‚ö†Ô∏è [WARNING] No se encontraron URLs de posts para {username}. Saliendo...")
            # Guardar el HTML para diagn√≥stico
            with open("page_source.html", "w", encoding="utf-8") as f:
                f.write(self.driver.page_source)
            print("[INFO] HTML de la p√°gina guardado en page_source.html para diagn√≥stico")
            return []
            
        # Ahora tenemos una lista de URLs de posts, procedemos a visitarlos uno por uno
        posts_data = []
        
        for i, post_url in enumerate(post_urls, 1):
            try:
                print(f"\nüì∏ [INFO] Procesando post {i}/{len(post_urls)}: {post_url}")
                
                # Visitar directamente la URL del post
                self.driver.get(post_url)
                self._human_delay(1, 2)  # Reducido para mayor velocidad
                
                # Extraer datos del post
                post_data = self._extract_post_data()
                
                if post_data:
                    posts_data.append(post_data)
                    print(f"‚úÖ Post {i} extra√≠do con √©xito.")
                else:
                    print(f"‚ö†Ô∏è [WARNING] No se pudo extraer datos del post {i}.")
                    
                # Breve pausa entre posts para evitar l√≠mites de rate
                if i < len(post_urls):
                    pause_time = random.uniform(0.5, 1.5)
                    print(f"[INFO] Pausa de {pause_time:.2f} segundos antes del siguiente post...")
                    time.sleep(pause_time)
                    
            except Exception as e:
                print(f"‚ùå [ERROR] Error procesando post {i}: {e}")
                traceback.print_exc()
                continue

        print(f"\n‚úÖ [INFO] Extracci√≥n completada: {len(posts_data)} posts obtenidos de {username}.")
        return posts_data

    def _extract_post_data(self):
        """Extrae datos del post abierto."""
        try:
            # Verificar que estamos en un post
            try:
                WebDriverWait(self.driver, 5).until(
                    EC.presence_of_element_located((By.XPATH, "//time"))
                )
            except TimeoutException:
                print("[ERROR] No se detect√≥ un post abierto.")
                return None

            post_url = self.driver.current_url
            print(f"[INFO] URL del post: {post_url}")

            # Extraer n√∫mero de likes - varios selectores posibles
            try:
                like_selectors = [
                    "//section//a[contains(@href, 'liked_by')]/span",
                    "//section//a[contains(@href, 'liked_by')]",
                    "//span[contains(text(), 'likes')]",
                    "//span[contains(text(), 'Me gusta')]",
                    "//span[contains(text(), 'Like')]"
                ]
                
                likes = "No disponible"
                for selector in like_selectors:
                    try:
                        likes_element = self.driver.find_element(By.XPATH, selector)
                        likes = likes_element.text.replace(",", "").strip()
                        if likes:
                            break
                    except:
                        continue
            except:
                likes = "No disponible"

            # Extraer fecha del post
            try:
                date_element = WebDriverWait(self.driver, 2).until(
                    EC.presence_of_element_located((By.TAG_NAME, "time"))
                )
                post_date = date_element.get_attribute("datetime")
            except:
                post_date = "No disponible"

            # Extraer comment del autor
            author_comment = self._author_comment()
            
            # Extraer comentarios
            comments = self._extract_comments()

            print(f"‚úÖ Post extra√≠do: {post_url} | Likes: {likes} | Fecha: {post_date}")

            return {
                "url_post": post_url,
                "author_comment": author_comment,
                "likes": likes,
                "fecha_post": post_date,
                "comentarios": comments
            }

        except Exception as e:
            print(f"[ERROR] No se pudo extraer datos del post: {e}")
            traceback.print_exc()
            return None

    def _extract_comments(self):
        """Extrae hasta 10 comentarios del post (reducido para mayor velocidad)."""
        comments_data = {}
        
        try:
            # M√∫ltiples intentos con diferentes selectores para encontrar comentarios
            comment_selectors = [
                "//ul//li[descendant::time]",  # Selector original
                "//ul//div[contains(@class, 'comment')]",  # Selector alternativo
                "//div[@role='dialog']//ul//li",  # Selector general para comentarios
                "//article//ul//li[.//a]"  # Selector basado en estructura com√∫n
            ]
            
            comment_elements = []
            
            for selector in comment_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    if elements:
                        comment_elements = elements[:10]  # Limitamos a 10 comentarios para mayor velocidad
                        print(f"[INFO] Encontrados {len(comment_elements)} comentarios con selector: {selector}")
                        break
                except:
                    continue
            
            if not comment_elements:
                print("[INFO] No se encontraron comentarios con ning√∫n selector.")
                return comments_data

            for comment in comment_elements:
                try:
                    # Extraer el nombre de usuario - m√∫ltiples intentos
                    username = "unknown"
                    try:
                        username = comment.find_element(By.XPATH, ".//img[contains(@alt, ' profile picture')]").get_attribute("alt").replace("'s profile picture", "")
                    except NoSuchElementException:
                        try:
                            username = comment.find_element(By.XPATH, ".//a").text
                        except:
                            pass

                    # Intentar extraer el texto del comentario - m√∫ltiples intentos
                    comment_text = None
                    try:
                        comment_text = comment.find_element(By.XPATH, ".//h3/following-sibling::div//span").text
                    except NoSuchElementException:
                        try:
                            comment_text = comment.find_element(By.XPATH, ".//span[not(contains(@class, 'timestamp'))]").text
                        except:
                            try:
                                # Intentar encontrar cualquier texto en el comentario
                                comment_text = comment.text
                            except:
                                pass

                    # Extraer la fecha del comentario
                    date = None
                    try:
                        date = comment.find_element(By.XPATH, ".//time").get_attribute("datetime")
                    except:
                        pass

                    # Extraer n√∫mero de likes
                    likes = "0 likes"
                    try:
                        likes = comment.find_element(By.XPATH, ".//button/span[contains(text(), 'like')]").text
                    except:
                        pass

                    # Verificar si hay imagen en el comentario
                    image_url = None
                    try:
                        image_elements = comment.find_elements(By.XPATH, ".//img[not(contains(@alt, ' profile picture'))]")
                        if image_elements:
                            image_url = image_elements[0].get_attribute("src")
                    except:
                        pass

                    # Guardar los datos
                    if comment_text:
                        comments_data[username] = (comment_text, date, likes)
                    elif image_url:
                        comments_data[username] = ("image/GIF", image_url, date, likes)
                    else:
                        comments_data[username] = ("unknown", date, likes)

                except Exception as e:
                    print(f"‚ö†Ô∏è Error extrayendo un comentario: {e}")
                    continue

        except Exception as e:
            print(f"‚ö†Ô∏è No se pudieron extraer comentarios: {e}")
            traceback.print_exc()

        return comments_data

    def _author_comment(self):
        """Extrae el comentario del autor del post en Instagram."""
        try:
            # M√∫ltiples selectores para encontrar el comentario del autor
            author_selectors = [
                "//ul//li[descendant::h1]",
                "//article//h1",
                "//div[@role='dialog']//div[contains(@class, 'caption')]",
                "//div[contains(@class, 'comment')]//div[contains(@class, 'owner')]",
                "//div[contains(@role, 'presentation')]//h1/..",  # Nuevo selector general
                "//article//div[.//h1]"  # Otro selector general
            ]
            
            for selector in author_selectors:
                try:
                    author_comment_element = self.driver.find_element(By.XPATH, selector)
                    
                    # Extraer nombre de usuario
                    username = "unknown"
                    try:
                        username_element = author_comment_element.find_element(By.XPATH, ".//img[contains(@alt, ' profile picture')]")
                        username = username_element.get_attribute("alt").replace("'s profile picture", "")
                    except:
                        try:
                            username_element = author_comment_element.find_element(By.XPATH, ".//a")
                            username = username_element.text
                        except:
                            pass
                    
                    # Extraer texto del comentario
                    comment_text = "No disponible"
                    try:
                        comment_text = author_comment_element.find_element(By.XPATH, ".//h1").text
                    except:
                        try:
                            comment_text = author_comment_element.find_element(By.XPATH, ".//span").text
                        except:
                            pass
                    
                    # Extraer hashtags
                    hashtags = []
                    try:
                        hashtags = [tag.text for tag in author_comment_element.find_elements(By.XPATH, ".//a[contains(@href, '/explore/tags/')]")]
                    except:
                        pass
                    
                    # Extraer menciones
                    mentions = []
                    try:
                        mentions = [mention.text for mention in author_comment_element.find_elements(By.XPATH, ".//a") if '@' in mention.text]
                    except:
                        pass
                    
                    return {
                        "author": username,
                        "comment": comment_text,
                        "hashtags": hashtags,
                        "mentions": mentions
                    }
                    
                except NoSuchElementException:
                    continue
            
            print("‚ö†Ô∏è No se encontr√≥ comentario del autor con ning√∫n selector.")
            return None

        except Exception as e:
            print(f"‚ö†Ô∏è Error al extraer comentario del autor: {e}")
            return None

    def save_to_csv(self, data, account):
        """Guarda los datos extra√≠dos en archivos CSV y JSON."""
        if not data:
            print("‚ö†Ô∏è No hay datos para guardar.")
            return

        # Convertir datos a formato tabular
        processed_data = []
        for post in data:
            post_data = {
                "url_post": post.get("url_post", ""),
                "likes": post.get("likes", ""),
                "fecha_post": post.get("fecha_post", ""),
                "author": post.get("author_comment", {}).get("author", "") if post.get("author_comment") else "",
                "comment": post.get("author_comment", {}).get("comment", "") if post.get("author_comment") else "",
                "hashtags": ", ".join(post.get("author_comment", {}).get("hashtags", [])) if post.get("author_comment") else "",
                "mentions": ", ".join(post.get("author_comment", {}).get("mentions", [])) if post.get("author_comment") else "",
            }
            
            # A√±adir comentarios en forma plana (como columnas adicionales)
            comentarios = post.get("comentarios", {})
            for i, (user, comment_data) in enumerate(comentarios.items(), 1):
                if i > 5:  # Limitamos a 5 comentarios por post para no tener demasiadas columnas
                    break
                post_data[f"comentario_{i}_usuario"] = user
                if isinstance(comment_data, tuple):
                    post_data[f"comentario_{i}_texto"] = comment_data[0]
                    if len(comment_data) > 1:
                        post_data[f"comentario_{i}_fecha"] = comment_data[1]
                    if len(comment_data) > 2:
                        post_data[f"comentario_{i}_likes"] = comment_data[2]
                else:
                    post_data[f"comentario_{i}_texto"] = str(comment_data)
            
            processed_data.append(post_data)

        # Crear DataFrame y guardar a CSV
        df = pd.DataFrame(processed_data)
        date_str = datetime.now().strftime("%d_%m_%Y")
        filename = f"instagram_posts_{account}_{date_str}.csv"

        df.to_csv(filename, index=False, encoding='utf-8')
        print(f"‚úÖ [INFO] Se guardaron {len(data)} filas en {filename}.")
        
        # Tambi√©n guardar una copia de los datos en JSON para preservar la estructura completa
        import json
        with open(f"instagram_posts_{account}_{date_str}.json", 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"‚úÖ [INFO] Se guard√≥ una copia en formato JSON para preservar la estructura completa.")

# =========================================================================
# MAIN
# =========================================================================
if __name__ == "__main__":
    # Puedes cambiar el nombre de la cuenta aqu√≠ o al inicio del archivo
    TARGET_ACCOUNT = ACCOUNT
    
    scraper = InstagramScraper()
    try:
        # 1) Login
        scraper.login()

        # 2) Comienza el scraper
        print(f"\n[INFO] Scrapeando {TARGET_ACCOUNT}")
        posts = scraper.scrape_profile(TARGET_ACCOUNT)

        print(f"[INFO] {TARGET_ACCOUNT} => {len(posts)} posts extra√≠dos")

        # 3) Guardar en CSV
        if posts:
            scraper.save_to_csv(posts, TARGET_ACCOUNT)
            print("\n[OK] Proceso completado con √©xito.")
        else:
            print("\n[WARNING] No se pudieron extraer posts. Verifica las razones en los logs y las capturas de pantalla.")

    except Exception as e:
        print("[CR√çTICO] Error en la ejecuci√≥n:", e)
        traceback.print_exc()
    finally:
        scraper.driver.quit()
        print("[INFO] Selenium cerrado.")